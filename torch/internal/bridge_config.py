# ******************************************************************************
# Copyright (C) 2023 Habana Labs, Ltd. an Intel Company
# All Rights Reserved.
#
# Unauthorized copying of this file or any element(s) within it, via any medium
# is strictly prohibited.
# This file contains Habana Labs, Ltd. proprietary and confidential information
# and is subject to the confidentiality and license agreements under which it
# was provided.
#
# ******************************************************************************

# The file is autogenerated by generate_env.py script based on
# env_flags.yaml configuration file. Do not edit this file directly!
import habana_frameworks.torch.internal._bridge_config_C as bc

from contextlib import contextmanager
@contextmanager
def env_setting(var, val):
    get_func = globals()['get_' + var.lower()]
    set_func = globals()['set_' + var.lower()]
    current = get_func()
    set_func(val)
    yield
    set_func(current)

def get_gc_kernel_path():
    return bc.get_gc_kernel_path()

def get_pt_habana_mem_log_filename():
    return bc.get_pt_habana_mem_log_filename()

def get_pt_hpu_graph_dump_prefix():
    return bc.get_pt_hpu_graph_dump_prefix()

# Specify in which mode should bridge dump intermediate graphs. Allowed values: <compile | eager | all>

# This flag takes effect only in non-lazy mode, meaning `PT_HPU_LAZY_MODE=0`
#  - `all` dumps both FX and JIT graphs from compile mode and JIT graphs from eager mode
#  - `compile` dumps both FX and JIT graphs from compile mode
#  - `compile_fx` dumps only FX graphs from compile mode
#  - `eager` dumps JIT graphs from eager mode
def get_pt_hpu_graph_dump_mode():
    return bc.get_pt_hpu_graph_dump_mode()

def get_pt_compilation_stats_path():
    return bc.get_pt_compilation_stats_path()

def get_pt_recipe_trace_path():
    return bc.get_pt_recipe_trace_path()

def get_pt_hpu_clustered_program_split_str():
    return bc.get_pt_hpu_clustered_program_split_str()

def get_pt_hpu_clustered_program_sched_str():
    return bc.get_pt_hpu_clustered_program_sched_str()

def get_pt_hpu_dynamic_min_policy_order():
    return bc.get_pt_hpu_dynamic_min_policy_order()

def get_pt_hpu_dynamic_max_policy_order():
    return bc.get_pt_hpu_dynamic_max_policy_order()

# It replaces PT_RECIPE_CACHE_PATH, PT_CACHE_FOLDER_DELETE and
#  PT_CACHE_FOLDER_SIZE_MB. PT_HPU_RECIPE_CACHE_CONFIG is a comma separated list
#  where params are encoded in the following way:
#  - 1st param: recipe cache directory path, if empty then disk cache is
#  disabled
#  - 2nd param: delete recipe cache one init, if set to true then PT bridge
#  clears recipe cache on init
#  - 3rd param: recipe cache max size in MB, if set to value > 0, PT bridge
#  keeps the size of cache dir under defined threshold.
#  Example: PT_HPU_RECIPE_CACHE_CONFIG=/tmp/recipe-cache,true,1024
def get_pt_hpu_recipe_cache_config():
    return bc.get_pt_hpu_recipe_cache_config()

# Env var 'PT_RECIPE_CACHE_PATH' to save compiled recipes to disk.
#  If proper path is set, disk cache is enabled for all compiled recipes.
def get_pt_recipe_cache_path():
    return bc.get_pt_recipe_cache_path()

def get_pt_hpu_clustered_program():
    return bc.get_pt_hpu_clustered_program()

def get_pt_hpu_clustered_program_enforce():
    return bc.get_pt_hpu_clustered_program_enforce()

def get_pt_hpu_lazy_mode():
    return bc.get_pt_hpu_lazy_mode()

def get_pt_hpu_lazy_acc_par_mode():
    return bc.get_pt_hpu_lazy_acc_par_mode()

def get_pt_hpu_lazy_acc_view_ops_mode():
    return bc.get_pt_hpu_lazy_acc_view_ops_mode()

def get_pt_hpu_use_syn_tensor_ids():
    return bc.get_pt_hpu_use_syn_tensor_ids()

def get_pt_hpu_mem_stats_dump():
    return bc.get_pt_hpu_mem_stats_dump()

def get_pt_hpu_print_stats():
    return bc.get_pt_hpu_print_stats()

def get_pt_hpu_print_stats_dump_freq():
    return bc.get_pt_hpu_print_stats_dump_freq()

def get_pt_hpu_print_stats_table():
    return bc.get_pt_hpu_print_stats_table()

def get_pt_hpu_internal_old_synapi():
    return bc.get_pt_hpu_internal_old_synapi()

def get_habana_use_persistent_tensor():
    return bc.get_habana_use_persistent_tensor()

def get_pt_hpu_lazy_eager_optim_cache():
    return bc.get_pt_hpu_lazy_eager_optim_cache()

def get_pt_hpu_eager_shape_agnostic_graph():
    return bc.get_pt_hpu_eager_shape_agnostic_graph()

def get_pt_hpu_lazy_eager_view_handling():
    return bc.get_pt_hpu_lazy_eager_view_handling()

def get_pt_hpu_enable_cache_metrics():
    return bc.get_pt_hpu_enable_cache_metrics()

def get_pt_hpu_enable_compile_thread():
    return bc.get_pt_hpu_enable_compile_thread()

def get_pt_hpu_enable_execution_thread():
    return bc.get_pt_hpu_enable_execution_thread()

def get_pt_hpu_enable_hpugraph_thread():
    return bc.get_pt_hpu_enable_hpugraph_thread()

def get_pt_hpu_enable_d2h_async_thread():
    return bc.get_pt_hpu_enable_d2h_async_thread()

def get_pt_hpu_queue_synlaunches():
    return bc.get_pt_hpu_queue_synlaunches()

def get_pt_hpu_inference_mode():
    return bc.get_pt_hpu_inference_mode()

def get_pt_hpu_inference_synapse_data_type_selection():
    return bc.get_pt_hpu_inference_synapse_data_type_selection()

def get_pt_hpu_inference_storage_override():
    return bc.get_pt_hpu_inference_storage_override()

def get_pt_hpu_enable_lazy_eager_execution_thread():
    return bc.get_pt_hpu_enable_lazy_eager_execution_thread()

def get_pt_hpu_enable_lazy_eager_launch_exec_thread():
    return bc.get_pt_hpu_enable_lazy_eager_launch_exec_thread()

def get_pt_hpu_eager_4_stage_pipeline_enable():
    return bc.get_pt_hpu_eager_4_stage_pipeline_enable()

def get_pt_hpu_enable_execution_thread_no_wait():
    return bc.get_pt_hpu_enable_execution_thread_no_wait()

def get_pt_hpu_thread_pool_queue_capacity():
    return bc.get_pt_hpu_thread_pool_queue_capacity()

def get_pt_enable_inter_host_caching():
    return bc.get_pt_enable_inter_host_caching()

def get_pt_do_not_lower_linear_op():
    return bc.get_pt_do_not_lower_linear_op()

def get_pt_enable_habana_caching():
    return bc.get_pt_enable_habana_caching()

def get_pt_enable_habana_streamasync():
    return bc.get_pt_enable_habana_streamasync()

def get_pt_enable_host_memory_cache():
    return bc.get_pt_enable_host_memory_cache()

def get_pt_enable_hcl_same_address_resolution():
    return bc.get_pt_enable_hcl_same_address_resolution()

def get_pt_enable_hcl_stream():
    return bc.get_pt_enable_hcl_stream()

def get_pt_habana_max_dma_copy_retry_count():
    return bc.get_pt_habana_max_dma_copy_retry_count()

def get_pt_habana_dma_copy_retry_delay():
    return bc.get_pt_habana_dma_copy_retry_delay()

def get_pt_hpu_max_recipe_submission_limit():
    return bc.get_pt_hpu_max_recipe_submission_limit()

def get_pt_hpu_eager_view_handling():
    return bc.get_pt_hpu_eager_view_handling()

def get_pt_hpu_eager_pipeline_enable():
    return bc.get_pt_hpu_eager_pipeline_enable()

def get_pt_hpu_eager_collective_pipeline_enable():
    return bc.get_pt_hpu_eager_collective_pipeline_enable()

def get_pt_hccl_slice_size_mb():
    return bc.get_pt_hccl_slice_size_mb()

def get_pt_habana_max_recipe_hit_count():
    return bc.get_pt_habana_max_recipe_hit_count()

# enable PT_STORE_SYNC if cs-timeouts are seen to perform host synchronization
#  before collectives
def get_pt_hpu_use_pt_store_sync():
    return bc.get_pt_hpu_use_pt_store_sync()

def get_pt_hpu_use_nw_stream_sync():
    return bc.get_pt_hpu_use_nw_stream_sync()

def get_pt_hpu_emulate_distributed():
    return bc.get_pt_hpu_emulate_distributed()

def get_pt_hpu_error_handler():
    return bc.get_pt_hpu_error_handler()

def get_pt_hpu_enable_permute_with_strided_view():
    return bc.get_pt_hpu_enable_permute_with_strided_view()

def get_pt_hpu_enable_slice_insert():
    return bc.get_pt_hpu_enable_slice_insert()

def get_pt_hpu_print_backtrace_on_signal():
    return bc.get_pt_hpu_print_backtrace_on_signal()

def get_pt_hpu_dump_ir_dot_graph():
    return bc.get_pt_hpu_dump_ir_dot_graph()

def get_pt_hpu_disable_instance_norm():
    return bc.get_pt_hpu_disable_instance_norm()

def get_pt_hpu_disable_async_collective():
    return bc.get_pt_hpu_disable_async_collective()

def get_pt_hpu_avoid_re_execute_graphs():
    return bc.get_pt_hpu_avoid_re_execute_graphs()

def get_pt_use_markstep():
    return bc.get_pt_use_markstep()

def get_pt_hpu_graph_dump():
    return bc.get_pt_hpu_graph_dump()

def get_pt_enable_synlaunch_time_capture():
    return bc.get_pt_enable_synlaunch_time_capture()

def get_pt_hpu_enable_debug_names():
    return bc.get_pt_hpu_enable_debug_names()

def get_pt_hpu_max_accum_size():
    return bc.get_pt_hpu_max_accum_size()

def get_pt_hpu_max_compound_op_size():
    return bc.get_pt_hpu_max_compound_op_size()

def get_pt_hpu_max_compound_op_sync():
    return bc.get_pt_hpu_max_compound_op_sync()

def get_pt_hpu_max_compound_op_size_ss():
    return bc.get_pt_hpu_max_compound_op_size_ss()

def get_pt_hpu_enable_stage_submission():
    return bc.get_pt_hpu_enable_stage_submission()

def get_pt_hpu_stage_submission_mode():
    return bc.get_pt_hpu_stage_submission_mode()

# flag to control recipe caching in non-eager backend
def get_pt_hpu_pgm_enable_cache():
    return bc.get_pt_hpu_pgm_enable_cache()

def get_pt_forced_tracing_mask():
    return bc.get_pt_forced_tracing_mask()

def get_pt_hpu_enable_eager_compiler():
    return bc.get_pt_hpu_enable_eager_compiler()

# separate flag to control recipe caching in eager backend
def get_pt_hpu_enable_eager_cache():
    return bc.get_pt_hpu_enable_eager_cache()

def get_pt_hpu_clear_scalar_map_on_markstep():
    return bc.get_pt_hpu_clear_scalar_map_on_markstep()

def get_pt_hpu_scalar_map_maxsize():
    return bc.get_pt_hpu_scalar_map_maxsize()

def get_pt_hpu_enable_lazy_collectives():
    return bc.get_pt_hpu_enable_lazy_collectives()

def get_pt_hpu_enable_sfg():
    return bc.get_pt_hpu_enable_sfg()

def get_pt_sbs():
    return bc.get_pt_sbs()

def get_pt_hpu_force_tanh_for_gelu():
    return bc.get_pt_hpu_force_tanh_for_gelu()

def get_pt_hpu_force_index_put_frontend_fallback():
    return bc.get_pt_hpu_force_index_put_frontend_fallback()

def get_pt_hpu_eager_index_put_bool_optimized():
    return bc.get_pt_hpu_eager_index_put_bool_optimized()

def get_pt_hpu_visualize_graph_index():
    return bc.get_pt_hpu_visualize_graph_index()

def get_pt_hpu_enable_nms_using_bnms_cguid():
    return bc.get_pt_hpu_enable_nms_using_bnms_cguid()

def get_pt_hpu_host_memory_threshold_percent():
    return bc.get_pt_hpu_host_memory_threshold_percent()

def get_pt_hpu_use_bn_fwd_in_gn_bwd():
    return bc.get_pt_hpu_use_bn_fwd_in_gn_bwd()

def get_pt_hpu_use_unsorted_scatter_add():
    return bc.get_pt_hpu_use_unsorted_scatter_add()

# enables fp8_143 variant instead of default fp8_152
#  for simulation purposes in algo team
def get_pt_use_fp8_143():
    return bc.get_pt_use_fp8_143()

# Dynamic shape related env variables
def get_pt_hpu_enable_dynamic_pass_fallback():
    return bc.get_pt_hpu_enable_dynamic_pass_fallback()

def get_pt_hpu_enable_min_max_as_current():
    return bc.get_pt_hpu_enable_min_max_as_current()

def get_pt_hpu_enable_refine_dynamic_shapes():
    return bc.get_pt_hpu_enable_refine_dynamic_shapes()

def get_pt_hpu_optim_dynamic_output_sif():
    return bc.get_pt_hpu_optim_dynamic_output_sif()

def get_pt_hpu_enable_jit_graph_name_hash():
    return bc.get_pt_hpu_enable_jit_graph_name_hash()

def get_pt_hpu_dev_enable_arange_host_tensor():
    return bc.get_pt_hpu_dev_enable_arange_host_tensor()

def get_pt_hpu_dev_enable_randperm_host_tensor():
    return bc.get_pt_hpu_dev_enable_randperm_host_tensor()

def get_pt_hpu_dev_enable_repeat_host_tensor():
    return bc.get_pt_hpu_dev_enable_repeat_host_tensor()

def get_pt_hpu_dev_enable_pad_host_tensor():
    return bc.get_pt_hpu_dev_enable_pad_host_tensor()

def get_pt_hpu_dev_enable_topk_using_cguid():
    return bc.get_pt_hpu_dev_enable_topk_using_cguid()

def get_pt_hpu_validate_compute_shape():
    return bc.get_pt_hpu_validate_compute_shape()

def get_pt_hpu_enable_zero_min():
    return bc.get_pt_hpu_enable_zero_min()

def get_pt_hpu_enable_disk_cache_for_dsd():
    return bc.get_pt_hpu_enable_disk_cache_for_dsd()

def get_pt_hpu_ds_time_improve_threshold_percent():
    return bc.get_pt_hpu_ds_time_improve_threshold_percent()

# Option to enable Fast shape Inference
def get_pt_hpu_enable_fast_shape_inference():
    return bc.get_pt_hpu_enable_fast_shape_inference()

# Option to enable and run Hybrid shape Inference
def get_pt_hpu_run_hybrid_sif():
    return bc.get_pt_hpu_run_hybrid_sif()

# Option to enable UniqueGraph feature
def get_pt_hpu_enable_unique_graph():
    return bc.get_pt_hpu_enable_unique_graph()

def get_pt_hpu_enable_broadcast_bucket_handling():
    return bc.get_pt_hpu_enable_broadcast_bucket_handling()

# H2D support for view ops
def get_pt_hpu_enable_h2d_dynamic_as_strided():
    return bc.get_pt_hpu_enable_h2d_dynamic_as_strided()

def get_pt_hpu_enable_h2d_dynamic_slice():
    return bc.get_pt_hpu_enable_h2d_dynamic_slice()

def get_pt_hpu_enable_dynamic_output_preallocate():
    return bc.get_pt_hpu_enable_dynamic_output_preallocate()

# Options to enable/disable std::copy to async thread for non blocking copy
def get_pt_hpu_enable_h2d_copy_async_thread():
    return bc.get_pt_hpu_enable_h2d_copy_async_thread()

# Minimum tensor size limit for non blocking copy default: Default 1 MB.
def get_pt_hpu_h2d_copy_min_tensor_size():
    return bc.get_pt_hpu_h2d_copy_min_tensor_size()

# Add scalars to list and copy using synMemCopyAsyncMultiple()
def get_pt_hpu_scalar_h2d_copy_multiple():
    return bc.get_pt_hpu_scalar_h2d_copy_multiple()

# Option to skip cache versioning mechanism.
#  This will skip the check of Libs and Env compatibility of serialized recipes
#  read from disk.
def get_pt_recipe_cache_ignore_version():
    return bc.get_pt_recipe_cache_ignore_version()

# Option to dump additional debug information to disk cache directory.
#  This works only with PT_RECIPE_CACHE_PATH set.
#  In the disk cache folder for every recipe, '<hash>.hash_content' files are
#  dumped. These files contain all the information that contribute to hash of a
#  given recipe and can be used i.e. in cases when graphs are expected to
#  produce exactly the same cache entires.
def get_pt_recipe_cache_dump_debug():
    return bc.get_pt_recipe_cache_dump_debug()

# Device memory related flags
def get_pt_hpu_initial_workspace_size():
    return bc.get_pt_hpu_initial_workspace_size()

def get_pt_habana_pool_size():
    return bc.get_pt_habana_pool_size()

def get_pt_hpu_pool_strategy():
    return bc.get_pt_hpu_pool_strategy()

def get_pt_habana_mem_log_level():
    return bc.get_pt_habana_mem_log_level()

def get_pt_hpu_pool_log_fragmentation_info():
    return bc.get_pt_hpu_pool_log_fragmentation_info()

def get_pt_enable_workspace_memory_shrink():
    return bc.get_pt_enable_workspace_memory_shrink()

def get_pt_hpu_pool_mem_fragment_json():
    return bc.get_pt_hpu_pool_mem_fragment_json()

def get_pt_enable_realtime_memory_logging():
    return bc.get_pt_enable_realtime_memory_logging()

def get_pt_enable_lightweight_memory_usage_logging():
    return bc.get_pt_enable_lightweight_memory_usage_logging()

# G1: 4 buffers, 6 communicators
#  internal slicing at 16 MB (16*4*6=384) per stream
def get_pt_hccl_memory_allowance_mb():
    return bc.get_pt_hccl_memory_allowance_mb()

def get_pt_enable_memory_defragmentation():
    return bc.get_pt_enable_memory_defragmentation()

def get_pt_enable_defragmentation_info():
    return bc.get_pt_enable_defragmentation_info()

def get_pt_hpu_pool_mem_enable_tensor_info():
    return bc.get_pt_hpu_pool_mem_enable_tensor_info()

def get_pt_hpu_pool_mem_alloc_retry_wait_ms():
    return bc.get_pt_hpu_pool_mem_alloc_retry_wait_ms()

def get_pt_hpu_pool_mem_acquire_perc():
    return bc.get_pt_hpu_pool_mem_acquire_perc()

def get_pt_hpu_pool_mem_alloc_enable_retry():
    return bc.get_pt_hpu_pool_mem_alloc_enable_retry()

def get_pt_hpu_enable_synapse_output_permute():
    return bc.get_pt_hpu_enable_synapse_output_permute()

def get_pt_hpu_enable_weight_cpu_permute():
    return bc.get_pt_hpu_enable_weight_cpu_permute()

def get_pt_hpu_enable_weight_hpu_permute():
    return bc.get_pt_hpu_enable_weight_hpu_permute()

def get_pt_hpu_max_permute_threshold():
    return bc.get_pt_hpu_max_permute_threshold()

# Option to enable numeric limits check for CPU tensors when downcasting
#  Ex long/double data type to int/float data type
def get_pt_hpu_enable_valid_data_range_check():
    return bc.get_pt_hpu_enable_valid_data_range_check()

# MultiUser stream - flag to force all ops to default stream
def get_pt_hpu_force_use_default_stream():
    return bc.get_pt_hpu_force_use_default_stream()

# gradient bucket feature
def get_pt_hpu_enable_gradient_view_layout_opt():
    return bc.get_pt_hpu_enable_gradient_view_layout_opt()

# Allow permutations on all reduce gradient bucket view outputs for PT2.0. This
#  is to improve multinode performance
def get_pt_hpu_eager_enable_gradient_view_layout_opt():
    return bc.get_pt_hpu_eager_enable_gradient_view_layout_opt()

# Allow stitching together of ops in PT Bridge Lowering
def get_pt_hpu_enable_compound_lowering_ops():
    return bc.get_pt_hpu_enable_compound_lowering_ops()

# matmul 3d,2d case handling with reshape
def get_pt_hpu_matmul3d_2d_reshape():
    return bc.get_pt_hpu_matmul3d_2d_reshape()

# Forward Graph running Hash
def get_pt_hpu_enable_graph_running_hash():
    return bc.get_pt_hpu_enable_graph_running_hash()

def get_pt_hpu_enable_validate_graph_running_hash():
    return bc.get_pt_hpu_enable_validate_graph_running_hash()

def get_pt_hpu_synchronous_acc_queue_flushing():
    return bc.get_pt_hpu_synchronous_acc_queue_flushing()

# Enable native support for tensors with INT64 datatype:
# - false - INT64 tensors are casted to INT32 on the device and all computations will be done in lower precision
#  - true - INT64 tensors are **not** casted to INT32 and computations will be done in higher precision

# This flag is supported only for non-lazy mode, meaning `PT_HPU_LAZY_MODE=0` and Gaudi2 or newer.
#  This flag is set to `false` by default as the default mode is lazy, meaning `PT_HPU_LAZY_MODE=1`
#  For the `PT_HPU_LAZY_MODE=0` the default value is `true`
def get_pt_enable_int64_support():
    return bc.get_pt_enable_int64_support()
def set_pt_enable_int64_support(value):
    bc.set_pt_enable_int64_support(value)

# Disable type promotion at bridge.
#  To disable it in bridge once CGUID supports type promotion.
def get_pt_disable_dtype_promotion():
    return bc.get_pt_disable_dtype_promotion()

# Wait for all futures during clean up.
def get_pt_wait_for_all_futures_in_cleanup():
    return bc.get_pt_wait_for_all_futures_in_cleanup()

# Enable flow events in tensorflow
def get_pt_tb_enable_flow_events():
    return bc.get_pt_tb_enable_flow_events()

# streams
def get_pt_hpu_enable_generic_stream():
    return bc.get_pt_hpu_enable_generic_stream()

def get_pt_hpu_enable_record_stream():
    return bc.get_pt_hpu_enable_record_stream()

def get_pt_hpu_enable_record_stream_noholder():
    return bc.get_pt_hpu_enable_record_stream_noholder()

def get_pt_hpu_use_launch_record_stream():
    return bc.get_pt_hpu_use_launch_record_stream()

# 2.0 support
def get_pt_hpu_eager_tensor_pool_size():
    return bc.get_pt_hpu_eager_tensor_pool_size()

def get_pt_hpu_enable_eager_tensor_timestamp():
    return bc.get_pt_hpu_enable_eager_tensor_timestamp()

def get_pt_hpu_sort_index_in_scatter_add():
    return bc.get_pt_hpu_sort_index_in_scatter_add()

# Determine number of files for logging rotation
def get_pt_log_file_amount():
    return bc.get_pt_log_file_amount()

def get_pt_towl_log_file_amount():
    return bc.get_pt_towl_log_file_amount()

def get_pt_towl_log_enable():
    return bc.get_pt_towl_log_enable()

def get_pt_towl_log_separated_file():
    return bc.get_pt_towl_log_separated_file()

def get_pt_towl_log_config():
    return bc.get_pt_towl_log_config()

# clang-format off
#  This is temporary variable, just to find out best
#  solution for accumulation thread
#  0 - default accumulation thread
#  1 - accumulation thread without spinning, could be slower but consume less
#  CPU time
#  2 - accumulation thread with spinning and mutexes is not used,
#  the fastest solution, but could consume more CPU time
def get_pt_hpu_acc_thread_version():
    return bc.get_pt_hpu_acc_thread_version()

# clang-format on
#  1GB per worker to save recipes to the disk
def get_pt_cache_folder_size_mb():
    return bc.get_pt_cache_folder_size_mb()

def get_pt_cache_folder_delete():
    return bc.get_pt_cache_folder_delete()

# Option to enable div the precise kernel.
#  This flag is used to enable or disable the use of the div precise kernel.
#  If this flag is not set, the div kernel will be used by default for div
#  operations without rounding mode; the div precise kernel will be used by
#  default for div operations with rounding mode.
def get_pt_hpu_enable_div_precise():
    return bc.get_pt_hpu_enable_div_precise()

# This flag enables new flow of random ops in torch.compile.
#  Random ops are replaced with custom wrappers containing seed tensor input.
#  This allows determinism without generating seed on CPU each iteration.
def get_pt_hpu_wrap_random_ops_compile():
    return bc.get_pt_hpu_wrap_random_ops_compile()

# This flag enable/Disable Eager Jit Cache. by default this is enabled.
def get_pt_hpu_enable_eager_jit_cache():
    return bc.get_pt_hpu_enable_eager_jit_cache()

# Mode of stochastic_rounding in cast_to_fp8 ops.
#  0 - maps round_mode to CAST_ROUND_SR,
#  1 - maps round_mode to CAST_ROUND_SFTZ - stochastic flush to zero.
def get_pt_hpu_stochastic_rounding_mode():
    return bc.get_pt_hpu_stochastic_rounding_mode()

# A comma-separated operator list can be set through this env to enable fallback operators to the CPU.
#  1) If PT_HPU_PLACE_ON_CPU is empty, fallback will be allowed for each
#  operator.
#  2) If PT_HPU_PLACE_ON_CPU contains `none`, fallback won't be allowed for any
#  operators.
#  3) Otherwise, fallback will be allowed for each operator defined in
#  PT_HPU_PLACE_ON_CPU.
def get_pt_hpu_place_on_cpu():
    return bc.get_pt_hpu_place_on_cpu()

# Enable negative values for indexing in index_put, gather ops.
def get_pt_hpu_enable_negative_indexing():
    return bc.get_pt_hpu_enable_negative_indexing()

# Compare num of parameters marked as const with this value in torch.compile mode
def get_pt_hpu_check_num_consts():
    return bc.get_pt_hpu_check_num_consts()

# Enable CompiledAutograd with hpu_backend in torch.compile.
def get_pt_hpu_enable_compiled_autograd():
    return bc.get_pt_hpu_enable_compiled_autograd()

# Cache communication groups creation instances. In model scenario
#  deepspeed and MDS creates the same process groups internally individaully.
#  This flag is to prevent duplication of communication group.
def get_pt_enable_comm_group_cache():
    return bc.get_pt_enable_comm_group_cache()

# Option to enable GPU Migration Toolkit
def get_pt_hpu_gpu_migration():
    return bc.get_pt_hpu_gpu_migration()

def get_pt_hpu_enable_allreduce_graph_split():
    return bc.get_pt_hpu_enable_allreduce_graph_split()

# Option to select to use same thread for synwaitstream and collective
#  or not. HCL might run into hang if the waitstream and collective happens
#  from different thread and its not synchronized. They expect
#  waitstream & collective call in synchronized manner.
def get_pt_hpu_waitstream_and_collective_use_same_thread():
    return bc.get_pt_hpu_waitstream_and_collective_use_same_thread()

